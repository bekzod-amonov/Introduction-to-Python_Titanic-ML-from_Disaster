{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62bbaaf5-fa97-40c8-9531-d162942123ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocess test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6561de-15bd-4449-a09d-ce97c324cd6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from titanic_survival_package.data_preprocessing import load_dataset, fill_missing_values, preprocess_test_data\n",
    "\n",
    "def test_load_dataset():\n",
    "    \"\"\"\n",
    "    Tests the loading of a dataset from a CSV file.\n",
    "\n",
    "    Ensures that the `load_dataset` function can successfully load data from a given file path\n",
    "    and that the resulting DataFrame is not empty.\n",
    "    \"\"\"\n",
    "    # Load training data and verify it's not empty\n",
    "    train_df = load_dataset('train.csv')\n",
    "    assert not train_df.empty, \"Failed to load training data. The DataFrame is empty.\"\n",
    "\n",
    "def test_fill_missing_values():\n",
    "    \"\"\"\n",
    "    Tests the handling of missing values within a dataset.\n",
    "\n",
    "    Verifies that the `fill_missing_values` function correctly fills or handles all missing values\n",
    "    within the dataset, ensuring no missing values remain.\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    train_df = load_dataset('train.csv')\n",
    "    \n",
    "    # Apply missing value handling\n",
    "    preprocessed_df = fill_missing_values(train_df.copy())\n",
    "    \n",
    "    # Assert no missing values remain\n",
    "    assert preprocessed_df.isnull().sum().max() == 0, \"Missing values were not properly handled.\"\n",
    "\n",
    "def test_preprocess_test_data():\n",
    "    \"\"\"\n",
    "    Tests the preprocessing of test data to align with the training data format.\n",
    "\n",
    "    Ensures that the `preprocess_test_data` function applies similar preprocessing steps to the test\n",
    "    data as were applied to the training data, including handling missing values and feature engineering,\n",
    "    and verifies that no unexpected missing values remain after preprocessing.\n",
    "    \"\"\"\n",
    "    # Load training data and test data\n",
    "    train_df = load_dataset('train.csv')\n",
    "    test_df = load_dataset('test.csv')\n",
    "    \n",
    "    # Preprocess test data using training data as a reference\n",
    "    test_preprocessed_df = preprocess_test_data(test_df.copy(), train_df)\n",
    "    \n",
    "    # Check for any remaining missing values\n",
    "    missing_values = test_preprocessed_df.isnull().sum()\n",
    "    print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Assert no missing values remain after preprocessing\n",
    "    assert test_preprocessed_df.isnull().sum().max() == 0, \"Test data still contains missing values after preprocessing.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ff273-af24-42aa-9702-902eebbdebe9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4149ee25-eb82-4539-ab2a-2dbf2bca7b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from titanic_survival_package.feature_engineering import create_family_size_feature, extract_and_process_titles, add_interaction_terms\n",
    "\n",
    "\n",
    "def test_create_family_size_feature():\n",
    "    \"\"\"\n",
    "    Tests the creation of a 'FamilySize' feature.\n",
    "\n",
    "    This function verifies that the `create_family_size_feature` function successfully adds a new column\n",
    "    'FamilySize' to the DataFrame, calculated as the sum of 'SibSp' (siblings/spouses aboard) and 'Parch'\n",
    "    (parents/children aboard) plus one for the passenger themselves.\n",
    "    \"\"\"\n",
    "    # Load the training dataset\n",
    "    train_df = pd.read_csv('train.csv')  # Adjust path as needed\n",
    "    \n",
    "    # Create 'FamilySize' feature\n",
    "    family_size_df = create_family_size_feature(train_df.copy())\n",
    "    \n",
    "    # Assert 'FamilySize' column exists\n",
    "    assert 'FamilySize' in family_size_df.columns, \"FamilySize feature not created.\"\n",
    "\n",
    "def test_extract_and_process_titles():\n",
    "    \"\"\"\n",
    "    Tests the extraction and processing of titles from passenger names.\n",
    "\n",
    "    Ensures that the `extract_and_process_titles` function can identify and extract titles from the 'Name'\n",
    "    column, simplify them into common categories, and encode these titles as dummy variables. Verifies that\n",
    "    a 'Title' column is created to reflect these changes.\n",
    "    \"\"\"\n",
    "    # Load the training dataset\n",
    "    train_df = pd.read_csv('train.csv')  # Adjust path as needed\n",
    "    \n",
    "    # Extract and process titles\n",
    "    titles_df = extract_and_process_titles(train_df.copy())\n",
    "    \n",
    "    # Assert 'Title' feature is created or processed\n",
    "    assert 'Title' in titles_df.columns, \"Title feature not created or processed.\"\n",
    "\n",
    "def test_add_interaction_terms():\n",
    "    \"\"\"\n",
    "    Tests the creation of interaction terms between features.\n",
    "\n",
    "    Verifies that the `add_interaction_terms` function successfully creates new features that are products\n",
    "    of existing features, specifically testing for the creation of an 'Age_Pclass' interaction term, which\n",
    "    combines 'Age' and 'Pclass' (ticket class).\n",
    "    \"\"\"\n",
    "    # Load the training dataset\n",
    "    train_df = pd.read_csv('train.csv')  # Adjust path as needed\n",
    "    \n",
    "    # Add interaction terms\n",
    "    interaction_df = add_interaction_terms(train_df.copy())\n",
    "    \n",
    "    # Assert interaction term 'Age_Pclass' is created\n",
    "    assert 'Age_Pclass' in interaction_df.columns, \"Interaction term 'Age_Pclass' not created.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1aefb-0d13-4aa2-9353-4bb8584799e1",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a8f329-f7ea-4932-9f10-7ceaa084e763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "def tune_hyperparameters_grid(model, X, y, parameters):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning using Grid Search Cross Validation. This method exhaustively \n",
    "    searches through all specified parameter combinations in 'parameters'.\n",
    "\n",
    "    Parameters:\n",
    "    - model (estimator): The machine learning model/estimator for which hyperparameters are to be optimized.\n",
    "    - X (array-like): Feature dataset used for training the model.\n",
    "    - y (array-like): Target values corresponding to 'X'.\n",
    "    - parameters (dict): Dictionary with parameters names (str) as keys and lists of parameter settings to try as values.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The best parameter setting found on the given data.\n",
    "    - float: Mean cross-validated score of the best_estimator.\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(model, parameters, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "def tune_hyperparameters_random(model, X, y, parameters, n_iter=10):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning using Randomized Search Cross Validation. This method samples \n",
    "    'n_iter' parameter settings from the specified 'parameters' distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - model (estimator): The machine learning model/estimator for which hyperparameters are to be optimized.\n",
    "    - X (array-like): Feature dataset used for training the model.\n",
    "    - y (array-like): Target values corresponding to 'X'.\n",
    "    - parameters (dict): Dictionary where keys are parameter names and values are distributions or lists of parameters to sample.\n",
    "    - n_iter (int, optional): Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The best parameter setting found on the given data.\n",
    "    - float: Mean cross-validated score of the best_estimator.\n",
    "    \"\"\"\n",
    "    random_search = RandomizedSearchCV(model, parameters, n_iter=n_iter, cv=5, scoring='accuracy', random_state=42)\n",
    "    random_search.fit(X, y)\n",
    "    return random_search.best_params_, random_search.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c6d8f-7f59-4f3b-8f84-25807654cff5",
   "metadata": {},
   "source": [
    "# Models test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bc0dbc-8a67-4a09-b7b7-203c66fcdf8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from titanic_survival_package.models import ModelTrainer, ModelEvaluator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_model_training_and_evaluation():\n",
    "    \"\"\"\n",
    "    Tests the training and evaluation of RandomForestClassifier using the ModelTrainer and ModelEvaluator classes.\n",
    "\n",
    "    This function demonstrates the process of initializing a model trainer with predefined hyperparameters,\n",
    "    training the model on a subset of the data for efficiency, and evaluating its performance on a separate validation set.\n",
    "    The function asserts the accuracy type to ensure the evaluation returns a floating-point number, indicative of the model's performance.\n",
    "    \"\"\"\n",
    "    # Define best hyperparameters obtained from previous tuning efforts\n",
    "    best_params = {\n",
    "        'n_estimators': [100],  # Example: Number of trees in the forest\n",
    "        'max_depth': [None],  # Example: Maximum depth of the trees\n",
    "        'min_samples_split': [2],  # Example: Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1]  # Example: Minimum number of samples required to be at a leaf node\n",
    "    }\n",
    "\n",
    "    # Initialize the ModelTrainer with a RandomForestClassifier and the best hyperparameters\n",
    "    model_trainer = ModelTrainer(RandomForestClassifier(random_state=42), best_params)\n",
    "\n",
    "    # Load training and validation sets here (not shown for brevity)\n",
    "    # For example purposes, using X_train, y_train, X_val, y_val\n",
    "\n",
    "    # Train the RandomForestClassifier model on a subset of the training data for quick testing\n",
    "    trained_model = model_trainer.train(X_train[:100], y_train[:100])\n",
    "\n",
    "    # Evaluate the trained model on a subset of the validation data\n",
    "    accuracy = ModelEvaluator.evaluate_model(trained_model, X_val[:20], y_val[:20])\n",
    "\n",
    "    # Assert the type of the accuracy to ensure the evaluation process returns a floating-point number\n",
    "    assert isinstance(accuracy, np.float64) or isinstance(accuracy, float), \"Model evaluation did not return accuracy as a floating-point number.\"\n",
    "\n",
    "    print(\"Model training and evaluation test passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4b496-8083-4755-a107-2bd9d3d81e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
